HW5: Markdown file
================
Kevin Liu
2023-11-12

# Problem 1

``` r
raw_homicide_data =
  read_csv("./data/homicide-data.csv") |> 
  janitor::clean_names()
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

**Describe the raw data**

- There are 52179 rows and 12 variables in the raw dataset.
- The 11 variables are “uid”, “reported_date”, “victim_last”,
  “victim_first”, “victim_race”, “victim_age”, “victim_sex”, “city”,
  “state”, “lat”, and “lon”.
  - Respectively their variable types: chr, num, chr, chr, chr, chr,
    chr, chr, chr, num, num, and chr.
  - The variables include full name of victim, their race, age, sex, the
    city and state and the coordinates of the homicide report and the
    date.

**Create a `city_state` variable**

``` r
raw_homicide_data = 
  raw_homicide_data |> 
  
  #Create variable "city_state" which is a concatenation of city + state separated by , .
  mutate(city_state = paste(city, state, sep = ", "))
```

**Summarize**

…within cities to obtain the total number of homicides and the number of
unsolved homicides.

``` r
sum_homicides =
  raw_homicide_data |> 
  
  #Group by city_state 
  group_by(city_state) |> 
  
  #Summarize total_homicides as # of data inputs in each city and num_unsolved the # of data with "Closed without arrest" and "Open/No arrest" as the string value for disposition
  summarize(total_homicides = n(), num_unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))) 

head(sum_homicides, 10)
```

    ## # A tibble: 10 × 3
    ##    city_state      total_homicides num_unsolved
    ##    <chr>                     <int>        <int>
    ##  1 Albuquerque, NM             378          146
    ##  2 Atlanta, GA                 973          373
    ##  3 Baltimore, MD              2827         1825
    ##  4 Baton Rouge, LA             424          196
    ##  5 Birmingham, AL              800          347
    ##  6 Boston, MA                  614          310
    ##  7 Buffalo, NY                 521          319
    ##  8 Charlotte, NC               687          206
    ##  9 Chicago, IL                5535         4073
    ## 10 Cincinnati, OH              694          309

**For City of Baltimore, MD…**

…use the prop.test function to estimate the proportion of homicides that
are unsolved; save the output of prop.test as an R object, apply the
broom::tidy to this object and pull the estimated proportion and
confidence intervals from the resulting tidy dataframe.

``` r
#Extract Baltimore_MD data from the summarized data aobve
baltimore_prop = 
  sum_homicides |> 
  filter(city_state == "Baltimore, MD")

#Extract the number of unsolved and total homicides from baltimore dataset
balt_unsolved = baltimore_prop$num_unsolved
balt_total = baltimore_prop$total_homicides

#Use prop.test function on the unsolved and total homicides from baltimore
balt_proptest = 
  prop.test(balt_unsolved, balt_total) |> 
  
  #Tidy the propotional test output
  broom::tidy()

#Extract the proportion and confidence interval from balt_proptes
balt_est = balt_proptest$estimate
balt_ci_low = balt_proptest$conf.low
balt_ci_high = balt_proptest$conf.high
```

- *Proportion of Homicides Unsolved in Baltimore, MD*: 0.6455607
- *95%CI*: (0.6275625, 0.6631599)

**Now Run `prop.test` on each city…**

…in your dataset, and extract both the proportion of unsolved homicides
and the confidence interval for each. Do this within a “tidy” pipeline,
making use of purrr::map, purrr::map2, list columns and unnest as
necessary to create a tidy dataframe with estimated proportions and CIs
for each city.

``` r
#Function that runs prop.test for each city
run_prop_test = function(x) {
  result = 
    prop.test(x$num_unsolved, x$total_homicides) |> 
    broom::tidy()
  
  
  tibble(city_state = x$city_state,
         est = result$estimate,
         ci_low = result$conf.low,
         ci_high = result$conf.high)
}

#Create a list to iterate over 
homicide_list = split(sum_homicides, 1:nrow(sum_homicides))

#Create empty list to fill with 
prop_list = vector("list",51)
for (i in 1:51) {
  prop_list[[i]] = run_prop_test(homicide_list[[i]])
}
```

    ## Warning in prop.test(x$num_unsolved, x$total_homicides): Chi-squared
    ## approximation may be incorrect

``` r
#Convert list into dataframe
prop_df = bind_rows(prop_list)

head(prop_df, 10)
```

    ## # A tibble: 10 × 4
    ##    city_state        est ci_low ci_high
    ##    <chr>           <dbl>  <dbl>   <dbl>
    ##  1 Albuquerque, NM 0.386  0.337   0.438
    ##  2 Atlanta, GA     0.383  0.353   0.415
    ##  3 Baltimore, MD   0.646  0.628   0.663
    ##  4 Baton Rouge, LA 0.462  0.414   0.511
    ##  5 Birmingham, AL  0.434  0.399   0.469
    ##  6 Boston, MA      0.505  0.465   0.545
    ##  7 Buffalo, NY     0.612  0.569   0.654
    ##  8 Charlotte, NC   0.300  0.266   0.336
    ##  9 Chicago, IL     0.736  0.724   0.747
    ## 10 Cincinnati, OH  0.445  0.408   0.483

**Create a plot**

…that shows the estimates and CIs for each city – check out
geom_errorbar for a way to add error bars based on the upper and lower
limits. Organize cities according to the proportion of unsolved
homicides.

``` r
  prop_df |> 
  #arrange by estimate ascending
  arrange(est) |> 
  #redfine city_state to factors and ordered based on their arrangement by est
  mutate(city_state = factor(city_state, levels = city_state)) |> 
  ggplot(aes(x = city_state, y = est)) + 
  geom_errorbar(aes(ymin = ci_low, ymax = ci_high)) +
  #make xaxis label more clear
  coord_flip()
```

![](p8105_hw5_kl3453_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

# Problem 2

\##Create a tidy dataframe…

…containing data from all participants, including the subject ID, arm,
and observations over time:

**Start with a dataframe containing all file names;**

``` r
#Retrieve directory names to iterate over (List)
file_names = list.files("./data", pattern = "^(con|exp)", full.names = TRUE)

#Retrieve subject name from the file names (List)
sub_names = substr(file_names, 8,13)

#Iterate all file names and read_csv using map
subject_data_list = map(file_names,read_csv, show_col_types = FALSE)

#Convert list to dataframe, add subject id + arm
subject_data = 
  bind_rows(subject_data_list) |> 
  mutate(subject_id = substr(sub_names,5,6)) |> 
  mutate(arm = substr(sub_names, 1,3)) |> 
  select(subject_id, arm, everything()) |> 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "value"
  ) |> 
  mutate(week = as.numeric(substr(week,6,6)))
```

**Make a spaghetti plot**

..showing observations on each subject over time, and comment on
differences between groups.

``` r
subject_data |> 
  ggplot(aes(x = week, y = value, color = arm, linetype = subject_id)) +
  geom_line()
```

![](p8105_hw5_kl3453_files/figure-gfm/unnamed-chunk-9-1.png)<!-- -->
*Difference*

- It seems like those in the experimental, have an overall increasing
  trend and are somewhat have higher absolute values compared to control
